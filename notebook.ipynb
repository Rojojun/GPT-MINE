{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb Cell 1\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m llm \u001b[39m=\u001b[39m HuggingFacePipeline\u001b[39m.\u001b[39mfrom_model_id(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     model_id\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     task\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtext-generation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m chain \u001b[39m=\u001b[39m prompt \u001b[39m|\u001b[39m llm\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hojunna/Desktop/Dev/HOJUN-GPT-STUDY/notebook.ipynb#W0sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m chain\u001b[39m.\u001b[39;49minvoke({\u001b[39m\"\u001b[39;49m\u001b[39mword\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mMurder\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/schema/runnable/base.py:1153\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1152\u001b[0m     \u001b[39mfor\u001b[39;00m i, step \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps):\n\u001b[0;32m-> 1153\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m step\u001b[39m.\u001b[39;49minvoke(\n\u001b[1;32m   1154\u001b[0m             \u001b[39minput\u001b[39;49m,\n\u001b[1;32m   1155\u001b[0m             \u001b[39m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   1156\u001b[0m             patch_config(\n\u001b[1;32m   1157\u001b[0m                 config, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mseq:step:\u001b[39;49m\u001b[39m{\u001b[39;49;00mi\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m   1158\u001b[0m             ),\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[1;32m   1160\u001b[0m \u001b[39m# finish the root run\u001b[39;00m\n\u001b[1;32m   1161\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/base.py:229\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvoke\u001b[39m(\n\u001b[1;32m    220\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    221\u001b[0m     \u001b[39minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    226\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    227\u001b[0m     config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m {}\n\u001b[1;32m    228\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m--> 229\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    230\u001b[0m             [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_input(\u001b[39minput\u001b[39;49m)],\n\u001b[1;32m    231\u001b[0m             stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    232\u001b[0m             callbacks\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcallbacks\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    233\u001b[0m             tags\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mtags\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    234\u001b[0m             metadata\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmetadata\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    235\u001b[0m             run_name\u001b[39m=\u001b[39;49mconfig\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mrun_name\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    236\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    237\u001b[0m         )\n\u001b[1;32m    238\u001b[0m         \u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n\u001b[1;32m    239\u001b[0m         \u001b[39m.\u001b[39mtext\n\u001b[1;32m    240\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/base.py:501\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    494\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    495\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    500\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/base.py:650\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    635\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    636\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    639\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    640\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    648\u001b[0m         )\n\u001b[1;32m    649\u001b[0m     ]\n\u001b[0;32m--> 650\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    651\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    652\u001b[0m     )\n\u001b[1;32m    653\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    654\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/base.py:538\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    537\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 538\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    539\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    540\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/base.py:525\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    516\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    517\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    521\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    522\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    523\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    524\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 525\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    526\u001b[0m                 prompts,\n\u001b[1;32m    527\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    528\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    529\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    530\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    531\u001b[0m             )\n\u001b[1;32m    532\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    533\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    534\u001b[0m         )\n\u001b[1;32m    535\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    536\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/langchain/llms/huggingface_pipeline.py:195\u001b[0m, in \u001b[0;36mHuggingFacePipeline._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    192\u001b[0m batch_prompts \u001b[39m=\u001b[39m prompts[i : i \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_size]\n\u001b[1;32m    194\u001b[0m \u001b[39m# Process batch of prompts\u001b[39;00m\n\u001b[0;32m--> 195\u001b[0m responses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpipeline(batch_prompts)\n\u001b[1;32m    197\u001b[0m \u001b[39m# Process each response in the batch\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39mfor\u001b[39;00m j, response \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(responses):\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/transformers/pipelines/text_generation.py:208\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    168\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[39m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(text_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/transformers/pipelines/base.py:1118\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39mif\u001b[39;00m is_list:\n\u001b[1;32m   1117\u001b[0m     \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[0;32m-> 1118\u001b[0m         final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_iterator(\n\u001b[1;32m   1119\u001b[0m             inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m         )\n\u001b[1;32m   1121\u001b[0m         outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(final_iterator)\n\u001b[1;32m   1122\u001b[0m         \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/transformers/pipelines/base.py:1071\u001b[0m, in \u001b[0;36mPipeline.get_iterator\u001b[0;34m(self, inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[39m# TODO hack by collating feature_extractor and image_processor\u001b[39;00m\n\u001b[1;32m   1070\u001b[0m feature_extractor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_processor\n\u001b[0;32m-> 1071\u001b[0m collate_fn \u001b[39m=\u001b[39m no_collate_fn \u001b[39mif\u001b[39;00m batch_size \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m pad_collate_fn(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer, feature_extractor)\n\u001b[1;32m   1072\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, num_workers\u001b[39m=\u001b[39mnum_workers, batch_size\u001b[39m=\u001b[39mbatch_size, collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[1;32m   1073\u001b[0m model_iterator \u001b[39m=\u001b[39m PipelineIterator(dataloader, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward, forward_params, loader_batch_size\u001b[39m=\u001b[39mbatch_size)\n",
      "File \u001b[0;32m~/Desktop/Dev/HOJUN-GPT-STUDY/env/lib/python3.11/site-packages/transformers/pipelines/base.py:132\u001b[0m, in \u001b[0;36mpad_collate_fn\u001b[0;34m(tokenizer, feature_extractor)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 132\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    133\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPipeline with tokenizer without pad_token cannot do batching. You can try to set it with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`pipe.tokenizer.pad_token_id = model.config.eos_token_id`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         )\n\u001b[1;32m    136\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m         t_padding_value \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mpad_token_id\n",
      "\u001b[0;31mValueError\u001b[0m: Pipeline with tokenizer without pad_token cannot do batching. You can try to set it with `pipe.tokenizer.pad_token_id = model.config.eos_token_id`."
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"A {word} is a\")\n",
    "\n",
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=\"gpt2\",\n",
    "    task=\"text-generation\",\n",
    "    pipeline_kwargs={\"max_new_tokens\": 150},\n",
    ")\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"word\": \"tomato\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
